{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning applications in research have proven to be of high-value, especially in process development. This reason is that the fruits of well defined ML/deep-learning models largely reflect the needs of our customers: optimization. We have clear outcome goals (cost of ownership, device performance and yield, etc), with robust process data, and our value to the company as researchers is to optimize process parameters to optimize performance as a function of cost. This presents as an ideal opportunity to utilize tools in machine learning.\n",
    "\n",
    "Common machine learning engineering largely requires two important pieces: 1) Build the model. 2) Build the cost function. But there is a third piece is critical to both machine learning and nanotech research: optimization. While you may have the ideal model and a becoming cost function, reconciliation of the two requires efficient optimization algorithm design if your model is going to converge in a way that doesn't waste your time (especially with the iterative nature of the data science development cycle). Gradient Descent stands as a staple of any researcher's tool kit, and for good reason: it's efficient and it's extensible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "As an example, we'll look at the Logistic Regression model defined in \"Logistic_Regression_Theory.ipynb\". Say we have our LR model (linear model + sigmoid function) and our loss function as notated below...\n",
    "\n",
    "$\\large z = w^Tx + b $\n",
    "\n",
    "$\\large\\hat{y} = a = \\sigma(z)$ (Using $a$ for brevity)\n",
    "\n",
    "$ \\large L(\\hat{y}, y) = - (y \\space log\\hat{y} \\space + \\space (1-y)\\space log(1-\\hat{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming two input variables ($w_1$, $w_2$, plus constant $b$), we can model the calculation flow of a single instance with a computation graph:\n",
    "\n",
    "$ \\large\\left[\\begin{array}{cc}x_1 & \\\\ w_1 & \\\\ x_2 & \\\\ w_2 & \\\\ b\\end{array}\\right] \\implies \\large [z = w_1x_1 + w_2x_2 + b] \\space\\to\\space [\\hat{y} = \\sigma(z)] \\space\\to\\space [L(a, y)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to know is the gradient of the loss function **with respect to a given input variable**. Knowing this gradient will tell us which \"direction\" to tune our weight/bias parameters, as a means of moving towards the minimum of the cost function.\n",
    "\n",
    "From here we compute the derivatives right-to-left across the computation graph, termed **backward propopgation**. Skipping over the calculus, basic chain-rule brings us to:\n",
    "\n",
    "$\\large \\frac{dL}{dz} = \\frac{dL}{da}\\frac{da}{dz} = a - y$\n",
    "\n",
    "$\\implies \\large \\frac{dL}{dw_1} = x_1 \\space dz = x_1(a - y)$\n",
    "\n",
    "$\\implies \\large \\frac{dL}{dw_2} = x_2 \\space dz = x_2(a - y)$\n",
    "\n",
    "$\\implies \\large \\frac{dL}{db} = dz \\space = a-y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This theory summarizes calculating loss of a **single** training instance, but to minimize the cost function over an entire training set requires these computations to be done across the training set. So for every predicted value in $a$, our Cost Function can be annotated\n",
    "\n",
    "$\\large J(w, b) = -\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)} \\space log(a)^{(i)} \\space + \\space (1-y^{(i)})\\space log(1-a^{(i)}]$\n",
    "\n",
    "We can iteratively compute the cost and graident of given weight/bias terms, updating our parameters at each iteration, with the aim of ultimately arriving where the gradients near zero.\n",
    "\n",
    "In the case of $J(w, b)$\n",
    "* $ \\large w = w - \\eta \\frac{\\partial{J(w, b)}}{\\partial{w}}$\n",
    "* $ \\large b = b - \\eta \\frac{\\partial{J(w, b)}}{\\partial{b}}$\n",
    "\n",
    "with $\\eta$ representing the learning rate (more on that later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code \n",
    "First, we'll incorporate the helper functions built in Logistic_Regression_Theory.ipynb to facilitate calculating sigmoid of linear function, and the forward propagation between the linear function and the cost function. Based on the calculus above, we can also add functionality for *backward* propagation from the cost function to the gradients of $w$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Helper function to compute the sigmoid of a given array\n",
    "    \n",
    "    input\n",
    "    -----\n",
    "    x: ndarray-like, output vector of linear function\n",
    "    \n",
    "    output\n",
    "    -----\n",
    "    s: ndarray-like, sigmoid of input\"\"\"\n",
    "    \n",
    "    sig = 1/(1+np.exp(-x))\n",
    "    return sig\n",
    "\n",
    "def init_wb(size, init_val=0):\n",
    "    \"\"\"\n",
    "    Function to initialize weights vector/bias term \n",
    "    \n",
    "    input\n",
    "    -----\n",
    "    size: int\n",
    "        length of weights vector\n",
    "    kind: int or float\n",
    "        value to init \n",
    "        \n",
    "    output\n",
    "    -----\n",
    "    w, b\n",
    "        \"\"\"\n",
    "    if init_val == 0:\n",
    "        w = np.zeros((size, 1))\n",
    "        b = 0\n",
    "    else:\n",
    "        w = np.ones((size, 1)) * init_val\n",
    "        b = init_val\n",
    "    return w, b\n",
    "\n",
    "def propagation(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Compute resulting cost function and gradients from\n",
    "    forward propagation and backward propagation respectively\n",
    "    \n",
    "    input\n",
    "    -----\n",
    "    X: ndarray-like, training set to fit\n",
    "    y: vector-like, X's corresponding target\n",
    "    w: weight vector\n",
    "    b: bias term\n",
    "    \n",
    "    output\n",
    "    ------\n",
    "    dict\n",
    "        dw: numeric, gradient of the weight vector\n",
    "        db: numeric, gradient of the bias vector\n",
    "        cost: vector-like, cost of weight, bias terms\n",
    "    \"\"\"\n",
    "    \n",
    "    samples = X.shape[1]\n",
    "    \n",
    "    # Forward Propagation (left to right)\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    cost = -(1/samples) * np.sum(y * np.log(A) + (1 - y)*(np.log(1 - A)))\n",
    "    \n",
    "    # Backward Propagation (right to left)\n",
    "    dw = (1/samples) * np.dot(X, (A - y).T)\n",
    "    db = (1/samples) * np.sum(A - y)\n",
    "    return {'dw': dw, 'db': db, 'cost':np.squeeze(cost)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our task is to *optimize* our parameters $w$ and $b$ in a way that minimizes our cost function. If we again consider the cost function\n",
    "\n",
    "$\\large J(w, b) = -\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)} \\space log(a)^{(i)} \\space + \\space (1-y^{(i)})\\space log(1-a^{(i)}]$\n",
    "\n",
    "This cost function is **convex** can be modeled as a surface across dimensions $w$ and $b$. In concept, this function can be visualized as a bowl-shape. The aim of gradient descent is finding the parameters $w$ and $b$ that place converge at the bottom of this bowl; the global minimum.\n",
    "\n",
    "In our `propagation` function, we effectively compute the gradients of our regressor parameters, coded as `dw` and `db`. To implement gradient descent, we must iteratively update our parameters at some rate $\\eta$ such that their values *descend* in the direction of their steepest gradient, hopefully arriving at some global minimum. This $\\eta$ is the learning rate, which controls the interval at which our parameters are changed.\n",
    "\n",
    "\n",
    "This can be coded simply as..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(X, y, w, b, n, eta):\n",
    "    \"\"\"\n",
    "    Function running gradient descent to optimize w and b\n",
    "    to minimize the cost function\n",
    "\n",
    "    input\n",
    "    -----\n",
    "    X: ndarray-like, training set to fit\n",
    "    y: vector-like, X's corresponding target\n",
    "    w: weight vector\n",
    "    b: bias term\n",
    "    n: number of iterations to loop\n",
    "    eta: learning rate\n",
    "    \n",
    "    output\n",
    "    ------\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        prop = propagation(X, y, w, b)\n",
    "        dw, db, cost = prop['dw'], prop['db'], prop['cost']\n",
    "        \n",
    "        w  = w - (eta * dw)\n",
    "        b = b - (eta * db)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return {'w':w, 'dw':dw, 'b':b, 'db':db, 'costs':costs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with our gradient descent function, we have all three pieces required in building a classifier. Our `grad_descent` function will return the optimal $w$ and $b$ for a given training set. But as our code stands right now, passing these parameters to our model will return a *probability* that an instance's target will be 1. For a classifier, we want to translate this probability to a definite outcome: 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(X, w, b):\n",
    "    \"\"\"\n",
    "    Predict binomial classification based on trained w, b\n",
    "    \n",
    "    input\n",
    "    -----\n",
    "    w: ndarray-like\n",
    "        weights vector\n",
    "    b: int or float\n",
    "        bias term\n",
    "    X: ndarray-like\n",
    "    \n",
    "    output\n",
    "    -----\n",
    "    y_pred: np.array\n",
    "        array of predictions to corresponding X instances\n",
    "        \"\"\"\n",
    "    samples = X.shape[1]\n",
    "    y_pred = np.zeros((1, samples))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        # Bin probabiliites into actual predictions\n",
    "        y_prob = A[0, i]\n",
    "        if y_prob > 0.5:\n",
    "            y_pred[:, i] = 1\n",
    "        else:\n",
    "            y_pred[:, i] = 0\n",
    "            \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we have all the parts of a logistic regression classifier. Look at Log_Reg_Classifier_from_Scratch.ipynb to see how we can assemble all of these parts into a fully functioning image classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
