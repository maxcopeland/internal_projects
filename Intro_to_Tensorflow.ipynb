{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Tensorflow\n",
    "\n",
    "In our industry, the lines between what constitutes Machine Learning, Deep Learning, AI, etc. are becoming blurry. This in part due to widely misunderstood theory behind the respective techniques and technologies, and certainly there's an element of tech branding behind the labels. But despite the fantastical images painted of this tech, it is widely accessible-- not just to developers, but to researchers as well. \n",
    "\n",
    "While machine learning has shown great value in consumer-facing fields, deep learning is making a case for itself in laboratory settings in providing insights into data with which traditional machine learning approaches struggle.\n",
    "\n",
    "In the Python universe, there are several well maintained, widely adopted deep learning frameworks. *Tensorflow* is arguably the most popular, guaging by GitHub stars and forks. Originally developed by the Google Brain team, it was open-sourced in 2015 and quickly rose to prominence in the deep learning community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataflow\n",
    "In TensorFlow, the user builds a *graph* that specifies the flow of computation. At a lower level, TF determines the dependencies between the operations and efficiently run them accordingly when executed. This is not unlike *dataflow* models in parallel and distributed computing. This framework has the benefit of being able to parallelize operations, and even distribute your model across devices and servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        x = tf.constant(5, name='x', dtype=tf.float32)\n",
    "        w=  tf.constant(1, name='w', dtype=tf.float32)\n",
    "        b = tf.constant(1, name='b', dtype=tf.float32)\n",
    "        z = w*x + b\n",
    "        sigmoid = 1 / (1 + tf.exp(-z))\n",
    "        tanh = tf.tanh(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, there are six nodes defined. These variables don't contain values, but rather associates the calculations therein with a computation graph. As the method implies, the first three objects `x`, `w`, and `b` are nodes to output constants. `name=` and `dtype=` are optional *source operations*, which set the attributes of the nodes that can be used to set the object's characteristics which can be used for later reference.\n",
    "\n",
    "Looking at the latter three objects, they depend on several of the preceding objects. The activation functions `sigmoid` and `tanh` cannot be evaluated without `z`, thus these functions have a *direct dependency* on `z`. `z` cannot be evaulated without the defined constants, on which `z` is also directly dependent, and `sigmoid` and `tanh` are *indirectly dependent*. When the operations are evaluated within the session, the computation graph used will reflect this order.\n",
    "\n",
    "\n",
    "Despite the above code, there has not been any calculations done. This is evident when trying to print a variable. What *has* been done is the instantiation of `Operation` objects (ops, for short). The output in printing these ops are `Tensor` objects, which are handles that make reference to the output that *will be* returned once the graph is executed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"truediv:0\", shape=(), dtype=float32)\n",
      "Tensor(\"Tanh:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid)\n",
    "print(tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Graph\n",
    "Behind the scenes, TensoFlow is associating the flow of computations to a `Graph` object. Once we import tensorflow, a default graph is formed to which subsequent node definitions are associated. When nodes are defined, (e.g. `x = tf.constant(5, name='x', dtype=tf.float32)`), the variable `x` represents a node within the default graph. Any number of graphs can be instantiated and executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.framework.ops.Graph object at 0x0000021B17F7FE10>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x0000021B3AFFCF28>\n"
     ]
    }
   ],
   "source": [
    "print(tf.get_default_graph())\n",
    "\n",
    "g2 = tf.Graph()\n",
    "\n",
    "with g2.as_default():\n",
    "    print(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In building this graph, each node is represented as a *tensor*, or ndarray, instead of a value. To evaluate any of these expresions, it must be executed in a *session*.\n",
    "\n",
    "\n",
    "Building a project in Tensorflow can largely be described as two phases: The construction phase and the execution phase:\n",
    "* The **construction phase** involves building the computation graph required for training the given model (above). \n",
    "* The **execution phase** involves training the model iteratively to optimize the model's parameters.\n",
    "\n",
    "In the below `with` clause: we are running a Tensorflow session. This \"session\" contains the user defined variables, and appropriately designates resources to evaluate the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid: 0.99753\n",
      "tanh: 0.99999\n"
     ]
    }
   ],
   "source": [
    "with g.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        sigmoid_val, tanh_val = sess.run([sigmoid, tanh])\n",
    "        print(\"sigmoid: {:.5f}\".format(sigmoid_val))        \n",
    "        print(\"tanh: {:.5f}\".format(tanh_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Session\n",
    "\n",
    "This is done in the *execution phase* of a basic tensorflow project. In calling `tf.Session()`, Tensorflow appropriately designates compute resources within the environment's CPU, GPU, etc, to execute the computations defined in the graph. Rather than instantiating a session variable and closing it after execution, we can run it in a `with` block.\n",
    "\n",
    "Within the open session, we can run a given node and print its output. The variables executed within the `run()` method are called *fetches*.\n",
    "\n",
    "While the `with` block convention is common practice in scripting and app development, in exploration and testing environments (i.e. notebooks, IDE's, etc), `InteractiveSession` can be launched in lieu of `Session` to install itself ass the default session, which can avoid needing to constantly refer to the session object(s). In this context, the `.eval()` method can be called on a node for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z= 34.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.Variable(5.)\n",
    "y = tf.Variable(2.)\n",
    "z = x**2 + 2*y + 5\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "print(f\"z= {z.eval()}\")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "While Tensor objects like `z` and `sigmoid` could be potentially filled with different values between different executions, `Variable` objects maintain their state within the graph. Rather than empty the tensor between computations, a `Variable`'s state persists between executions. So if a variable $a$ is fit to a linear model with a value of say 2.0, *the value of $a$ will be 2.0 the next time the graph is executed*. This is important for iterating on variables between executions when optimizing a model. \n",
    "\n",
    "\n",
    "A variable object requires two steps in a program: \n",
    "1. calling the `Variable()` function \n",
    "2. initializing its value(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 1 x-init: 5\n",
      "name: x:0 \n",
      "\n",
      "Session 1 +5: 10\n",
      "name: add:0 \n",
      "\n",
      "Session 2 Init: 10\n",
      "name: add:0 \n",
      "\n",
      "Re-run Function...\n",
      "\n",
      "Session 1 x-init: 5\n",
      "name: x_1:0 \n",
      "\n",
      "Session 1 +5: 10\n",
      "name: add_1:0 \n",
      "\n",
      "Session 2 Init: 10\n",
      "name: add_1:0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "g = tf.Graph()\n",
    "\n",
    "def var_init():\n",
    "    with g.as_default():\n",
    "        # Define the variable\n",
    "        x = tf.Variable(5, name=\"x\")\n",
    "        # Create variable initializer\n",
    "        init = tf.global_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            # Initialize variable\n",
    "            sess.run(init)\n",
    "            print(\"Session 1 x-init:\",sess.run(x))\n",
    "            print(\"name:\",x.name, \"\\n\")\n",
    "\n",
    "            x = x + 5\n",
    "            print(\"Session 1 +5:\",sess.run(x))\n",
    "            print(\"name:\",x.name, '\\n')\n",
    "\n",
    "        with tf.Session() as sess2:\n",
    "            sess2.run(init)\n",
    "            print(\"Session 2 Init:\",sess2.run(x))\n",
    "            print(\"name:\",x.name, '\\n')        \n",
    "\n",
    "var_init()\n",
    "print(\"Re-run Function...\\n\")\n",
    "var_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that between different sessions, the value stored in the Variable `x` persists between different sessions and maintains a fixed state in the graph. Running the same initialization returns a different name for the variable, and a new variable is created in the graph every time. There is no teardown after the session ends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders\n",
    "Utilization of placeholders (`np.zeros`, `np.ones`, etc) in Python in the context of scientific computing is commonplace, as it makes storage of values much more efficient on compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "895 µs ± 47.5 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "33.2 ms ± 699 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "def fast_store(n):\n",
    "    x = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        x[i] = i\n",
    "    return x\n",
    "\n",
    "def slow_store(n):\n",
    "    x = np.array([])\n",
    "    for i in range(n):\n",
    "        np.append(x, i)\n",
    "    return x\n",
    "\n",
    "%timeit -n 10 fast_store(10000)\n",
    "%timeit -n 10 slow_store(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow's `placeholder` object functions similarly, functioning as an empty data structure to be filled with data at execution, with a key difference that defining its shape is optional. A common practice is to define the placeholder's shape as `(None, n_x)`, where number of features of the trained data $n_x$ is known and leaving the number of samples $m$ unspecified (as batch size can vary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=(?, 15) dtype=float32>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = tf.placeholder(tf.float32, shape=(None, 15))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outs = [[10. 10. 10. 10. 10.]\n",
      " [10. 10. 10. 10. 10.]\n",
      " [10. 10. 10. 10. 10.]\n",
      " [10. 10. 10. 10. 10.]\n",
      " [10. 10. 10. 10. 10.]]\n"
     ]
    }
   ],
   "source": [
    "X_init = np.zeros((5, 5))\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=(5, 5))\n",
    "    tens = X + 10\n",
    "    with tf.Session() as sess:\n",
    "        outs = sess.run(tens, feed_dict={X: X_init})\n",
    "\n",
    "print(\"outs = {}\".format(outs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Graph Example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, take a simple linear model.\n",
    "Below is an evaluation of the normal equation to fit a linear regression model on training data $X$ and target $y$,  we can find least squares through the normal equation \n",
    "\n",
    "$W = (X^TX)^{-1}X^Ty$\n",
    "\n",
    "where $X$ is of shape ($m$, $n$). Within this computation graph, tensorflow parallelizes the calculations within the matrix multiplication on the user's GPU card (given you've installed the supporting Tensorflow GPU support packet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 33.389023]\n",
      " [ 81.66528 ]\n",
      " [112.97624 ]\n",
      " [ 15.458442]\n",
      " [ 20.357716]\n",
      " [ 58.611958]\n",
      " [ 18.960642]\n",
      " [ 97.52006 ]\n",
      " [ 63.445244]\n",
      " [ 86.58184 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Get the data\n",
    "X, y = make_regression(n_features=10, n_informative=10, bias=1)\n",
    "m, n = X.shape\n",
    "b = np.ones((m, 1))\n",
    "\n",
    "# Add bias term\n",
    "X_b = np.c_[b, data]\n",
    "X_b_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Define variables\n",
    "X = tf.constant(X_b_scaled, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(y.reshape(-1, 1), dtype=tf.float32, name='y')\n",
    "\n",
    "# Build normal equation\n",
    "X_transpose = tf.transpose(X)\n",
    "W = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(X_transpose, X)), X_transpose), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    W_vals = W.eval()\n",
    "\n",
    "print(W_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "With the use of `Variable`s and `placeholder`s, we can effectively optimize the parameters of a learning model. To do so, we first define a cost function (see \"ML_Cost_Functions.ipynb\"). Using the example above, a basic appraoch would be to use an MSE cost, and iteratively calculate the gradient of the cost with respect to the parameters and updating `W` accordingly (see \"Gradient_Descent_Theory.ipynb\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: MSE = 1352.946\n",
      "Epoch 100: MSE = 50.47625\n",
      "Epoch 200: MSE = 22.143654\n",
      "Epoch 300: MSE = 20.871607\n",
      "Epoch 400: MSE = 20.800337\n",
      "Epoch 500: MSE = 20.795948\n",
      "Epoch 600: MSE = 20.795664\n",
      "Epoch 700: MSE = 20.795643\n",
      "Epoch 800: MSE = 20.795643\n",
      "Epoch 900: MSE = 20.795643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-2.2154120e-05],\n",
       "       [-1.9091724e-05],\n",
       "       [ 1.1751610e-05],\n",
       "       [ 2.0417933e-06],\n",
       "       [-8.0235886e-06],\n",
       "       [ 5.7001848e-06],\n",
       "       [-2.3571036e-05],\n",
       "       [ 4.2903166e-06],\n",
       "       [-3.4671698e-06],\n",
       "       [ 3.6011684e+01]], dtype=float32)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_regression(n_features=10, n_informative=1, bias=1, random_state=1)\n",
    "\n",
    "def lin_reg(X, y, eta=0.01, epochs=1000, optimizer=None):\n",
    "    \"\"\"\n",
    "    Functiont to fit \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "    # Get the data\n",
    "    m, n = X.shape\n",
    "    b = np.ones((m, 1))\n",
    "\n",
    "    # Add bias term\n",
    "    X_b = np.c_[b, data]\n",
    "    X_b_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Define variables\n",
    "    X = tf.constant(X_b_scaled, dtype=tf.float32, name=\"X\")\n",
    "    X_transpose = tf.transpose(X)\n",
    "    y = tf.constant(y.reshape(-1, 1), dtype=tf.float32, name='y')\n",
    "\n",
    "    # Create variable for weights\n",
    "    W_init = tf.random.uniform([n, 1], -1, 1)\n",
    "    W = tf.Variable(W_init, name='W')\n",
    "\n",
    "    # Build cost functions\n",
    "    y_pred = tf.matmul(X, W, name=\"predictions\")\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "    # Build optimizer\n",
    "    if optimizer == None:\n",
    "        gradients = 2/m * tf.matmul(X_transpose, error)\n",
    "        optim_op = tf.assign(W, W - eta * gradients)\n",
    "    else:\n",
    "        optimizer = optimizer(learning_rate=eta)\n",
    "        optim_op = optimizer.minimize(mse)\n",
    "\n",
    "    # Initialize the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            if epoch % 100 == 0:\n",
    "                print(\"Epoch {}:\".format(epoch), \"MSE =\", mse.eval())\n",
    "            sess.run(optim_op)\n",
    "\n",
    "        W_best = W.eval()\n",
    "    return W_best\n",
    "\n",
    "lin_reg(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autodiff\n",
    "\n",
    "Computing gradients of parameters like above is essential in optimizing machine learning models, but hard coding these derivatives can obviously become unwieldy for even moderately complex algorithms, especially in networks that require back-propagation across several layers. Rather than define your own optimization by assigning a new value to a variable, TensorFlow has built-in optimizers within its `train` module, which can be implemented by defining the optimizer, and passing your cost function to the method of interest (e.g. `.minimize()`). These features are built on TensorFlow's autodiff functionality which enables efficient computation of gradients across a variable, given an op. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: MSE = 1254.07\n",
      "Epoch 100: MSE = 50.86708\n",
      "Epoch 200: MSE = 22.216215\n",
      "Epoch 300: MSE = 20.877363\n",
      "Epoch 400: MSE = 20.800753\n",
      "Epoch 500: MSE = 20.795975\n",
      "Epoch 600: MSE = 20.795664\n",
      "Epoch 700: MSE = 20.795645\n",
      "Epoch 800: MSE = 20.795643\n",
      "Epoch 900: MSE = 20.795643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-2.1615038e-05],\n",
       "       [-1.5646498e-05],\n",
       "       [ 1.5039540e-05],\n",
       "       [ 5.0544027e-06],\n",
       "       [-7.9438887e-06],\n",
       "       [ 4.9725713e-06],\n",
       "       [-2.4574336e-05],\n",
       "       [ 7.3482374e-06],\n",
       "       [-2.9006189e-06],\n",
       "       [ 3.6011684e+01]], dtype=float32)"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer\n",
    "lin_reg(X, y, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out \"Deep_Learning_in_TensorFlow.ipynb\" for a look at how to use the TF framework to build a deep layer perceptron."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_env]",
   "language": "python",
   "name": "conda-env-tf_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
